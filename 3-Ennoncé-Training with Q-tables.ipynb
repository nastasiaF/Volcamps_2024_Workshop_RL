{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 Entrainement d'un agent avec une Q-table\n",
    "\n",
    "Nous allons créer agent d'apprentissage par renforcement qui apprend à jouer au jeu du pendu en utilisant la méthode Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Créer la policie : la fonction epsilon_greedy_policy**\n",
    "\n",
    "Vous devez implémenter une fonction epsilon_greedy_policy qui sélectionne une action selon une stratégie d'exploration-exploitation basée sur la méthode epsilon-greedy. L'agent choisit une action soit en explorant de manière aléatoire, soit en exploitant les connaissances déjà acquises (c'est-à-dire en sélectionnant l'action ayant la meilleure valeur Q pour l'état donné).\n",
    "\n",
    "**Paramètres d'entrée** :\n",
    "\n",
    "- state: un tuple contenant deux éléments :\n",
    "  - state[0]: l'**indice** de l'état courant dans la Q-table.\n",
    "  - state[1]: une liste ou un tableau indiquant les actions interdites (lettres déjà jouées). Les indices des lettres déjà jouées sont `True`, si la lettre n'a pas encore était jouée ce sera `False`.\n",
    "- q_table: la table des des valeurs d'état-action Q utilisée pour stocker les valeurs Q pour chaque paire état-action, qui lui permet à l'agent d'estimer les récompenses associées à chaque action dans un état donné.\n",
    "- epsilon: le paramètre epsilon pour la politique epsilon-greedy, par défaut à 0.3. Il permet d'ajuster la proportion d'actions aléatoires (exploration) par rapport aux actions basées sur les valeurs maximales dans la Q-Table (exploitation).\n",
    "\n",
    "**Fonctionnement Stratégie epsilon-greedy :**\n",
    "\n",
    "Avec une probabilité égale à epsilon,  on est dans une phase d'**exploration**, l'agent choisit une action aléatoire parmi les actions disponibles (indices de 0 à 25, 26 lettres.). \n",
    "\n",
    "Sinon, nous **exploitons** la Q-table disponible, l'agent sélectionne l'action ayant la valeur Q maximale pour l'état courant dans la Q-Table (exploitation).\n",
    "\n",
    "  _Gestion des actions interdites_ : Si certaines actions sont interdites (c'est-à-dire déjà essayées), elles doivent être \"masquées\" (en leur attribuant une valeur très basse, comme -inf), afin de s'assurer que l'agent ne choisit pas ces actions.\n",
    "  \n",
    "```python\n",
    "if np.random.uniform(0, 1) < epsilon:\n",
    "  return np.random.choice(26)\n",
    "```\n",
    "**Retour** \n",
    " La fonction retourne l'index de l'action choisie : 0 pour A, 1 pour B,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Initialiser les paramètres et les variables de stockage**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Nombre d'épisodes :**\n",
    "\n",
    "  ```python\n",
    "  n_episode = 50000\n",
    "  ```\n",
    "\n",
    "  Cela définit le nombre total d'épisodes pour lesquels l'agent va s'entraîner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Historique des récompenses et des actions :**\n",
    "\n",
    "  ```python\n",
    "  hist_reward = np.zeros(n_episode)         # Récompenses obtenues par épisode\n",
    "  hist_actions = np.zeros((26, n_episode))  # Lettres choisies à chaque étape\n",
    "  ```\n",
    "\n",
    "  - `hist_reward` stocke la récompense totale obtenue à chaque épisode.\n",
    "  - `hist_actions` enregistre l'ordre des lettres choisies par l'agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Valeurs initiales :**\n",
    "\n",
    "  ```python\n",
    "  init_value_qtable = 0.3 \n",
    "  epsilon = 0.3\n",
    "  ```\n",
    "\n",
    "  - `init_value_qtable` est la valeur initiale de chaque entrée dans la Q-table.\n",
    "  - `epsilon` est le paramètre pour la politique ε-greedy, contrôlant l'équilibre entre exploration et exploitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **Initialiser la Q-table**\n",
    "\n",
    "- **Création de la Q-table :**\n",
    "\n",
    "  ```python\n",
    "  q_table = np.full((28**max_length, 26), init_value_qtable, dtype=float)\n",
    "  ```\n",
    "\n",
    "  - La Q-table est une matrice qui associe chaque état possible à une valeur pour chaque action possible (ici, les 26 lettres de l'alphabet).\n",
    "  - `28**max_length` représente le nombre total d'états possibles, en tenant compte des lettres déjà devinées et des positions possibles dans le mot.\n",
    "  - `max_length` correspond à la longueur maximale des mots utilisés dans le jeu. Pour commencer, je vous recommande des petits mots, par exemple 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Le script d'apprentissage \n",
    "\n",
    "#### 4.1 **Initialiser l'environnement**\n",
    "\n",
    "- **Création de l'environnement du pendu :**\n",
    "\n",
    "  ```python\n",
    "  env = HangedManEnv(max_word_size=max_length)\n",
    "  ```\n",
    "\n",
    "  - `HangedManEnv` est une classe qui représente l'environnement du jeu du pendu.\n",
    "  - `max_word_size` définit la longueur maximale des mots que l'agent devra deviner.\n",
    "\n",
    "#### 4.2 . **Boucle principale d'apprentissage**\n",
    "\n",
    "- Une itération par épisode :*\n",
    "\n",
    "\n",
    "##### 4.2.1 **Réinitialiser l'environnement pour chaque épisode**\n",
    "\n",
    "- **Réinitialisation :**\n",
    "\n",
    "  ```python\n",
    "  obs, info = env.reset()\n",
    "  terminated = False\n",
    "  step = 1\n",
    "  ```\n",
    "\n",
    "  - `env.reset()` réinitialise l'environnement et renvoie l'observation initiale.\n",
    "  - `terminated` indique si l'épisode est terminé.\n",
    "  - `step` est un compteur pour le nombre d'actions dans l'épisode.\n",
    "\n",
    "##### 4.2.3 **Conversion de l'observation en indice d'état :**\n",
    "\n",
    "L'état est la version codée ou transformée de l'observation, qui peut être un index ou une représentation plus simple (discrète ou continue) que l'algorithme peut utiliser pour apprendre une politique. dans notre cas, avec une Q-table l'état représente un index.\n",
    "\n",
    "  ```python\n",
    "  state_index = np.dot(obs[0], [28**i for i in range(max_length)])\n",
    "  ```\n",
    "\n",
    "  - Cette opération convertit l'observation actuelle en un indice unique pour accéder à la Q-table.\n",
    "\n",
    "Pour aller plus loin : aller voir le notebook : 3-Observation_to_state.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.2.4 **Boucle interne jusqu'à la fin de l'épisode**\n",
    "\n",
    "\n",
    "- Choisir une action selon la politique ε-greedy (epsilon_greedy_policy) définie\n",
    "\n",
    "- Exécuter l'action choisie dans l'environnement :\n",
    "\n",
    "\n",
    "- **Mettre à jour la Q-table**\n",
    "\n",
    "  - **Calculer l'indice du nouvel état :**\n",
    "  - **Trouver la meilleure action future avec la q_table:**\n",
    "  - **Calculer la cible de TD (Temporal Difference) :**\n",
    "        ```python\n",
    "        td_target = reward + q_table[next_state_index][best_next_action]\n",
    "        ```\n",
    "- **Calculer l'erreur de TD :**\n",
    "\n",
    "   L'erreur de Temparal Différence mesure l'ajustement que l'agent doit faire dans ses estimations des valeurs des états ou des actions en fonction de la récompense qu'il vient de recevoir. Elle permet à l'agent de corriger progressivement ses estimations de valeur à chaque étape, au lieu d'attendre la fin d'un épisode.\n",
    "\n",
    "  ```python\n",
    "  td_error = td_target - q_table[state_index][action]\n",
    "  ```\n",
    "\n",
    "- **Mettre à jour la Q-table :**\n",
    "\n",
    "  ```python\n",
    "  q_table[state_index][action] += q_table[state_index][action] + td_error\n",
    "  ```\n",
    "\n",
    "  Cette mise à jour suit l'équation de mise à jour du Q-learning sans facteur d'apprentissage explicite (alpha = 1).\n",
    "\n",
    "- **Préparer pour le prochain état :**\n",
    "\n",
    "  ```python\n",
    "  state_index = next_state_index\n",
    "  ```\n",
    "\n",
    "- Enregistrer les données pour l'analyse\n",
    "\n",
    "- Sauvegarder les actions et les récompenses :\n",
    "\n",
    "####  **Fin de l'épisode**\n",
    "\n",
    "- La boucle while se termine lorsque l'agent a deviné le mot ou épuisé ses tentatives.\n",
    "\n",
    "### **Fermer l'environnement**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
